{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be46939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydot\n",
    "import graphviz\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l2\n",
    "from tensorflow.keras import utils\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten, Dense, Embedding, Activation, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95bcf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvolutionalNet(vocabulary_size, embedding_dimension, input_length, embedding_weights=None):\n",
    "    \n",
    "    model = Sequential()\n",
    "    if embedding_weights is None:\n",
    "        model.add(Embedding(vocabulary_size, embedding_dimension, input_length=input_length, trainable=False))\n",
    "    else:\n",
    "        model.add(Embedding(vocabulary_size, embedding_dimension, input_length=input_length, weights=[embedding_weights], trainable=False))\n",
    "\n",
    "    model.add(Convolution1D(32, 2, kernel_regularizer=l2(0.005)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Convolution1D(32, 2, kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Convolution1D(32, 2, kernel_regularizer=l2(0.001)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(MaxPooling1D(17))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1, kernel_regularizer=l2(0.001))) #bias=True,\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "783df746",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 20\n",
    "EMBEDDING_DIMENSION = 30\n",
    "\n",
    "def words_to_indices(inverse_vocabulary, words):\n",
    "    return [inverse_vocabulary[word] for word in words]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    vocabulary = open(\"../data/vocabulary.txt\").read().split(\"\\n\")\n",
    "    inverse_vocabulary = dict((word, i) for i, word in enumerate(vocabulary))\n",
    "\n",
    "    clickbait = open(\"../data/clickbait.preprocessed.txt\").read().split(\"\\n\")\n",
    "    clickbait = pad_sequences([words_to_indices(inverse_vocabulary, sentence.split()) for sentence in clickbait], maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    genuine = open(\"../data/genuine.preprocessed.txt\").read().split(\"\\n\")\n",
    "    genuine = pad_sequences([words_to_indices(inverse_vocabulary, sentence.split()) for sentence in genuine], maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    X = np.concatenate([clickbait, genuine], axis=0)\n",
    "    y = np.array([[1] * clickbait.shape[0] + [0] * genuine.shape[0]], dtype=np.int32).T\n",
    "    p = np.random.permutation(y.shape[0])\n",
    "    X = X[p]\n",
    "    y = y[p]\n",
    "\n",
    "    X_train, X_test, y_train, y_test =  train_test_split(X, y, stratify=y)\n",
    "\n",
    "    embedding_weights = np.load(\"../models/embeddings.npy\")\n",
    "    params = dict(vocabulary_size=len(vocabulary), embedding_dimension=EMBEDDING_DIMENSION, input_length=SEQUENCE_LENGTH, embedding_weights=embedding_weights)\n",
    "    model = ConvolutionalNet(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63aa8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "model,\n",
    "to_file=\"model.png\",\n",
    "show_shapes=True,\n",
    "show_dtype=False,\n",
    "show_layer_names=True,\n",
    "rankdir=\"TB\",\n",
    "expand_nested=True,\n",
    "dpi=96,\n",
    "layer_range=None,\n",
    "show_layer_activations=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d39bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
